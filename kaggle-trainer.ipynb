{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "controlled-brave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'file_path': 'D:\\\\WORK\\\\freelance\\\\agnet\\\\dataset\\\\face_kaggle.csv', 'image_base_path': 'D:\\\\WORK\\\\freelance\\\\agnet', 'target_fields': ['age'], 'batch_size': 2, 'workers': 2, 'n_splits': 1, 'train_size': 0.8, 'random_state': 7, 'image_size': 768, 'scale_factor': 10, 'output_dim': 1}, 'model': {'name': 'AGNet', 'base_model': 'resnet50', 'base_model_weights': 'ResNet50_Weights', 'description': 'age and gender classification neural network', 'lr': 0.01, 'fp': 'fp32', 'model_version': 'age_estimation_resnet34', 'epochs': 128, 'tqdm_enable': True, 'output_dim': 1}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WORK\\freelance\\agnet\\agnet\\train.py:60: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(open(args.config_file,'r'))\n",
      "D:\\Research Lab\\computer vision\\env\\image\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Research Lab\\computer vision\\env\\image\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "\n",
      "  0%|          | 0/109456 [00:00<?, ?it/s]\n",
      "  0%|          | 0/109456 [00:01<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\WORK\\freelance\\agnet\\agnet\\train.py\", line 88, in <module>\n",
      "    run(args)\n",
      "  File \"D:\\WORK\\freelance\\agnet\\agnet\\train.py\", line 84, in run\n",
      "    trainer.fit(train_dl, val_dl, epochs=config['model']['epochs'])\n",
      "  File \"D:\\WORK\\freelance\\agnet\\agnet\\model\\helper.py\", line 110, in fit\n",
      "    self.train_step(dataloader, epoch)\n",
      "  File \"D:\\WORK\\freelance\\agnet\\agnet\\model\\helper.py\", line 46, in train_step\n",
      "    if y_h.shape[-1]>1:\n",
      "AttributeError: 'NoneType' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "base_model\n",
      "AGNet(\n",
      "  (base_model): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=128, out_features=32, bias=True)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=32, out_features=1, bias=True)\n",
      "      (9): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "None\n",
      "********************\n",
      "EPOCH: 0 started\n"
     ]
    }
   ],
   "source": [
    "!python agnet/train.py -c agnet/config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "animated-breakdown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.0+cpu'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "norwegian-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "liked-spine",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_l_32-c7638314.pth\" to C:\\Users\\deepa/.cache\\torch\\hub\\checkpoints\\vit_l_32-c7638314.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b7596839614fd19c7718aba4a67340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/1.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = torchvision.models.vit_l_32(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "operational-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Sequential(*[\n",
    "                nn.Linear(1024, 1),\n",
    "                nn.ReLU()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "digital-falls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(3, 1024, kernel_size=(32, 32), stride=(32, 32))\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_12): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_13): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_14): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_15): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_16): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_17): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_18): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_19): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_20): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_21): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_22): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_23): EncoderBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "Conv2d(3, 1024, kernel_size=(32, 32), stride=(32, 32))\n",
      "Encoder(\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): Sequential(\n",
      "    (encoder_layer_0): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_1): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_2): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_3): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_4): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_5): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_6): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_7): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_8): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_9): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_10): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_11): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_12): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_13): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_14): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_15): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_16): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_17): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_18): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_19): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_20): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_21): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_22): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_23): EncoderBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "        (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Sequential(\n",
      "  (encoder_layer_0): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_1): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_2): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_3): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_4): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_5): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_6): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_7): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_8): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_9): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_10): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_11): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_12): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_13): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_14): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_15): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_16): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_17): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_18): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_19): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_20): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_21): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_22): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer_23): EncoderBlock(\n",
      "    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (self_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): MLPBlock(\n",
      "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (act): GELU()\n",
      "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "MLPBlock(\n",
      "  (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (act): GELU()\n",
      "  (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "  (linear_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (dropout_2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=1024, out_features=4096, bias=True)\n",
      "GELU()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "Sequential(\n",
      "  (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n",
      "Linear(in_features=1024, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for i in model.modules():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "mexican-abraham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1,3,768,768)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-wonder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-holder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./agnet\n",
    "!git clone https://github.com/deepakdhull80/agnet.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/kaggle/input/face-image-dataset/face_kaggle.csv\")\n",
    "df['file_path'] = df['file_path'].map(lambda x: x.replace('\\\\','/').replace(\"dataset/\",''))\n",
    "df.to_csv(\"/kaggle/working/face_kaggle.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./agnet/agnet/config.yaml', 'r') as file:\n",
    "    config = yaml.full_load(file)\n",
    "\n",
    "config['data']['batch_size'] = 128\n",
    "config['model']['tqdm_enable'] = False\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./agnet/agnet/config.yaml', 'w') as file:\n",
    "    yaml.dump(config, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python agnet/agnet/train.py \\\n",
    "        -c agnet/agnet/config.yaml \\\n",
    "        -vv=base_resnet34 \\\n",
    "        -d=cuda \\\n",
    "        -fp=/kaggle/working/face_kaggle.csv \\\n",
    "        -ip=/kaggle/input/age-prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
